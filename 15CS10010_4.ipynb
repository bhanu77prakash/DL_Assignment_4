{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All neccessary imports and context initialization\n",
    "\n",
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import mxnet\n",
    "mx.random.seed(1)\n",
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document(doco):\n",
    "    punctuation = string.punctuation + '\\n\\n';\n",
    "    punc_replace = ''.join([' ' for s in punctuation]);\n",
    "    doco_clean = doco.replace('-', ' ');\n",
    "    doco_alphas = re.sub(r'\\W +', '', doco_clean)\n",
    "    trans_table = string.maketrans(punctuation, punc_replace);\n",
    "    doco_clean = ' '.join([word.translate(trans_table) for word in doco_alphas.split(' ')]);\n",
    "    doco_clean = doco_clean.split(' ');\n",
    "    doco_clean = [word.lower() for word in doco_clean if len(word) > 0];\n",
    "    return doco_clean;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to convert an indexed sentence to one hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hots(numerical_list, vocab_size):\n",
    "    result = nd.zeros((len(numerical_list), vocab_size), ctx=ctx)\n",
    "    for i, idx in enumerate(numerical_list):\n",
    "        if(idx == -1):\n",
    "            continue\n",
    "        result[i, idx] = 1.0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to add padding to a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(dataset):\n",
    "    maxlen = max([len(row) for row in dataset])\n",
    "    for i in range(len(dataset)):\n",
    "        dataset[i] = dataset[i]+[-1]*(maxlen - len(dataset[i]))\n",
    "#         for j in range(maxlen - len(dataset[i])):\n",
    "#             dataset[i].append(-1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y_linear):\n",
    "    if(len(y_linear.shape) == 1):\n",
    "        lin = (y_linear-nd.max(y_linear))\n",
    "        exp = nd.exp(lin)\n",
    "        partition = nd.sum(exp)\n",
    "        return exp / partition\n",
    "    lin = (y_linear-nd.max(y_linear, axis=1).reshape((-1,1))) # shift each row of y_linear by its max\n",
    "    exp = nd.exp(lin)\n",
    "    partition =nd.sum(exp, axis=1).reshape((-1,1))\n",
    "    return exp / partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(yhat, y):\n",
    "    return - nd.mean(nd.sum(y * nd.log(yhat), axis=0, exclude=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Cross Entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_ce_loss(outputs, labels):\n",
    "    assert(len(outputs) == len(labels))\n",
    "    total_loss = 0.\n",
    "    for (output, label) in zip(outputs,labels):\n",
    "        total_loss = total_loss + cross_entropy(output, label)\n",
    "    return total_loss / len(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with gradient clipping implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(params, lr):\n",
    "    # Gradient clipping\n",
    "    for i in range(len(params)):\n",
    "        mxnet.ndarray.clip(params[i].grad, a_min = -10, a_max = 10, out=params[i].grad)\n",
    "    # Back Prop\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A - With one hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'str' has no attribute 'maketrans'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6a1304997003>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclean_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-c479704bdbd0>\u001b[0m in \u001b[0;36mclean_document\u001b[0;34m(doco)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdoco_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdoco_alphas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\W +'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoco_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrans_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpunc_replace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdoco_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans_table\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoco_alphas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdoco_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoco_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'str' has no attribute 'maketrans'"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "\n",
    "with open(\"train.txt\", \"r\") as file:\n",
    "    train_data = file.readlines()\n",
    "    train_data = [clean_document(doc) for doc in train_data]\n",
    "    \n",
    "with open(\"test.txt\", \"r\") as file:\n",
    "    test_data = file.readlines()\n",
    "    test_data = [clean_document(doc) for doc in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding the SOS and EOS delimiters to the data\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    train_data[i] = [\"<SOS>\"]+train_data[i]\n",
    "    train_data[i] = train_data[i]+[\"<EOS>\"]\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    test_data[i] = [\"<SOS>\"]+test_data[i]\n",
    "    test_data[i] = test_data[i]+[\"<EOS>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocab: 6692\n"
     ]
    }
   ],
   "source": [
    "total_words = []\n",
    "for row in train_data:\n",
    "    total_words += row\n",
    "for row in test_data:\n",
    "    total_words += row\n",
    "    \n",
    "word_list = list(set(total_words))\n",
    "vocab_size = len(word_list)\n",
    "print(\"Length of vocab: %s\" % vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating dictionary (Don't run it for simply testing without training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "for e, word in enumerate(word_list):\n",
    "    word_dict[word] = e\n",
    "np.save('./word_dict_model_1.npy', word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dictionary for simply testing without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dict = np.load('./weights/word_dict_model_1.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for row in train_data:\n",
    "    X_train.append([word_dict[x] for x in row])\n",
    "\n",
    "X_train_list = X_train.copy()\n",
    "X_test = []\n",
    "for row in test_data:\n",
    "    X_test.append([word_dict[x] for x in row])\n",
    "\n",
    "\n",
    "X_train = padding(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for row in X_train:\n",
    "    dataset.append(one_hots(row[:-1], vocab_size))\n",
    "\n",
    "temp = nd.zeros((len(dataset), dataset[0].shape[0], dataset[0].shape[1]), ctx=ctx)\n",
    "for i in range(len(temp)):\n",
    "    temp[i] = dataset[i]\n",
    "dataset = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sequences in dataset:  3610\n",
      "# of batches:  56\n",
      "Shape of data set:  (56, 19, 64, 6692)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "seq_length = len(dataset[0])\n",
    "print('# of sequences in dataset: ', len(dataset))\n",
    "num_batches = len(dataset) // batch_size\n",
    "print('# of batches: ', num_batches)\n",
    "train_data = dataset[:num_batches*batch_size].reshape((batch_size, num_batches, seq_length, vocab_size))\n",
    "train_data = nd.swapaxes(train_data, 0, 1)\n",
    "train_data = nd.swapaxes(train_data, 1, 2)\n",
    "print('Shape of data set: ', train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_label = []\n",
    "for i in X_train:\n",
    "    X_label.append(i[1:])\n",
    "    \n",
    "labels = []\n",
    "for row in X_label:\n",
    "    labels.append(one_hots(row, vocab_size))\n",
    "\n",
    "temp = nd.zeros((len(labels), labels[0].shape[0], labels[0].shape[1]), ctx=ctx)\n",
    "for i in range(len(temp)):\n",
    "    temp[i] = labels[i]\n",
    "labels = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data set:  (56, 19, 64, 6692)\n"
     ]
    }
   ],
   "source": [
    "train_labels = labels[:num_batches*batch_size].reshape((batch_size, num_batches, seq_length, vocab_size))\n",
    "train_labels = nd.swapaxes(train_labels, 0, 1)\n",
    "train_labels = nd.swapaxes(train_labels, 1, 2)\n",
    "print('Shape of data set: ', train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For training model, run this cell (For testing only, run below cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = vocab_size\n",
    "num_hidden = 128\n",
    "num_outputs = vocab_size\n",
    "Wxh_model_1 = nd.random_normal(shape=(num_inputs,num_hidden), ctx=ctx) * .01\n",
    "Whh_model_1 = nd.random_normal(shape=(num_hidden,num_hidden), ctx=ctx) * .01\n",
    "bh_model_1 = nd.random_normal(shape=num_hidden, ctx=ctx) * .01\n",
    "Why_model_1 = nd.random_normal(shape=(num_hidden,num_outputs), ctx=ctx) * .01\n",
    "by_model_1 = nd.random_normal(shape=num_outputs, ctx=ctx) * .01\n",
    "\n",
    "params_model_1 = [Wxh_model_1, Whh_model_1, bh_model_1, Why_model_1, by_model_1]\n",
    "\n",
    "for param in params_model_1:\n",
    "    param.attach_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For simply testing without training run this cell instead of above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "num_inputs = vocab_size\n",
    "num_hidden = 128\n",
    "num_outputs = vocab_size\n",
    "\n",
    "Wxh_model_1 = mxnet.ndarray.load(\"./weights/Wxh_model_1\")[0]\n",
    "Whh_model_1 = mxnet.ndarray.load(\"./weights/Whh_model_1\")[0]\n",
    "bh_model_1 = mxnet.ndarray.load(\"./weights/bh_model_1\")[0]\n",
    "Why_model_1 = mxnet.ndarray.load(\"./weights/Why_model_1\")[0]\n",
    "by_model_1 = mxnet.ndarray.load(\"./weights/by_model_1\")[0]\n",
    "\n",
    "params_model_1 = [Wxh_model_1, Whh_model_1, bh_model_1, Why_model_1, by_model_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_rnn_model_1(inputs, state):\n",
    "    outputs = []\n",
    "    h = state\n",
    "    for X in inputs:\n",
    "        h_linear = nd.dot(X, Wxh_model_1) + nd.dot(h, Whh_model_1) + bh_model_1\n",
    "        h = nd.tanh(h_linear)\n",
    "        yhat_linear = nd.dot(h, Why_model_1) + by_model_1\n",
    "        yhat = softmax(yhat_linear)\n",
    "        outputs.append(yhat)\n",
    "    return (outputs, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_last_word_1(in_dataset, ground_data):\n",
    "    true = 0\n",
    "    count = 0\n",
    "    for i in range(len(in_dataset)):\n",
    "        if(len(in_dataset[i]) <= 3):\n",
    "            continue\n",
    "        state = nd.zeros(shape=(num_hidden), ctx=ctx)\n",
    "        data_one_hot = in_dataset[i]\n",
    "        with autograd.record():\n",
    "            outputs, state = simple_rnn_model_1(data_one_hot, state)\n",
    "        true += int(outputs[-3].asnumpy().argmax() == ground_data[i][-3])\n",
    "        count+=1\n",
    "\n",
    "\n",
    "    print(\"Result: \", true, count, true/float(count)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "### (For simply testing the pretrained model, please run below cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "for row in X_test:\n",
    "    test_dataset.append(one_hots(row[:-1], vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 173.75654649734497\n",
      "Epoch 1. Loss: 173.3448269367218\n",
      "Epoch 2. Loss: 172.903146982193\n",
      "Epoch 3. Loss: 172.48332262039185\n",
      "Epoch 4. Loss: 172.10226702690125\n",
      "Epoch 5. Loss: 171.70755910873413\n",
      "Epoch 6. Loss: 171.3070297241211\n",
      "Epoch 7. Loss: 170.9063436985016\n",
      "Epoch 8. Loss: 170.50190210342407\n"
     ]
    }
   ],
   "source": [
    "epochs = 125\n",
    "\n",
    "learning_rate = 0.9\n",
    "for e in range(epochs):\n",
    "    over_loss = 0\n",
    "    state = nd.zeros(shape=(batch_size, num_hidden), ctx=ctx)\n",
    "    for i in range(num_batches):\n",
    "        data_one_hot = train_data[i]\n",
    "        label_one_hot = train_labels[i]\n",
    "        with autograd.record():\n",
    "            outputs, state = simple_rnn_model_1(data_one_hot, state)\n",
    "            loss = average_ce_loss(outputs, label_one_hot)\n",
    "            loss.backward()\n",
    "        SGD(params_model_1, learning_rate)\n",
    "        over_loss += np.mean(loss.asnumpy()[0])\n",
    "    print(\"Epoch %s. Loss: %s\" % (e, over_loss))\n",
    "    if ((e+1) % 25 == 0):\n",
    "        test_last_word_1(test_dataset, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "for row in X_train_list:\n",
    "    train_dataset.append(one_hots(row[:-1], vocab_size))\n",
    "    \n",
    "print(\"Train Results: (True, number_of_samples, percentage)\")\n",
    "test_last_word_1(train_dataset, X_train_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "for row in X_test:\n",
    "    test_dataset.append(one_hots(row[:-1], vocab_size))\n",
    "\n",
    "print(\"Test Results: (True, number_of_samples, percentage)\")\n",
    "test_last_word_1(test_dataset, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model parameters (please don't run while testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mxnet.ndarray.save(\"Wxh_model_1\", Wxh_model_1)\n",
    "mxnet.ndarray.save(\"Whh_model_1\", Whh_model_1)\n",
    "mxnet.ndarray.save(\"bh_model_1\", bh_model_1)\n",
    "mxnet.ndarray.save(\"Why_model_1\", Why_model_1)\n",
    "mxnet.ndarray.save(\"by_model_1\", by_model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part -B: With pretrained Fasttext embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train.txt\", \"r\") as file:\n",
    "    train_data = file.readlines()\n",
    "    train_data = [clean_document(doc) for doc in train_data]\n",
    "    \n",
    "with open(\"test.txt\", \"r\") as file:\n",
    "    test_data = file.readlines()\n",
    "    test_data = [clean_document(doc) for doc in test_data]\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    train_data[i] = [\"<SOS>\"]+train_data[i]\n",
    "    train_data[i] = train_data[i]+[\"<EOS>\"]\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    test_data[i] = [\"<SOS>\"]+test_data[i]\n",
    "    test_data[i] = test_data[i]+[\"<EOS>\"]\n",
    "    \n",
    "file = open(\"cleaned_data.txt\", \"w\")\n",
    "for sent in train_data+test_data:\n",
    "    for word in sent:\n",
    "        file.write(word+\" \")\n",
    "    file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fasttext\n",
    "# embed_size = 300\n",
    "# model = fasttext.skipgram('cleaned_data.txt', 'model', dim=embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import io\n",
    "# embed_size = 300\n",
    "# def load_vectors(fname):\n",
    "#     fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "#     n, d = map(int, fin.readline().split())\n",
    "#     data = {}\n",
    "#     for line in fin:\n",
    "#         tokens = line.rstrip().split(' ')\n",
    "#         data[tokens[0]] = map(float, tokens[1:])\n",
    "#     return data\n",
    "\n",
    "# model = load_vectors(\"wiki-news-300d-1M.vec\")\n",
    "# model_words = model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "model = dict()\n",
    "embed_size = 300\n",
    "f = open('wiki-news-300d-1M.vec')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0].lower()\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    model[word] = coefs\n",
    "    \n",
    "model_words = list(model.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocab: 6694\n"
     ]
    }
   ],
   "source": [
    "total_words = []\n",
    "for row in train_data:\n",
    "    total_words += row\n",
    "for row in test_data:\n",
    "    total_words += row\n",
    "    \n",
    "word_list = list(set(total_words))\n",
    "vocab_size = len(word_list)\n",
    "print(\"Length of vocab: %s\" % vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating dictionary (Don't run it for simply testing without training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "for e, word in enumerate(word_list):\n",
    "    word_dict[word] = e\n",
    "\n",
    "temp = []\n",
    "for i in model_words:\n",
    "    if(i in word_dict):\n",
    "        temp.append(i)\n",
    "        \n",
    "\n",
    "model_words = temp\n",
    "np.save('./word_dict_model_2.npy', word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dictionary for simply testing without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = np.load('./weights/word_dict_model_2.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_word_dict = {v: k for k, v in word_dict.items()}\n",
    "\n",
    "X_train = []\n",
    "for row in train_data:\n",
    "    X_train.append([word_dict[x] for x in row])\n",
    "\n",
    "X_train_list = X_train[:]\n",
    "X_test = []\n",
    "for row in test_data:\n",
    "    X_test.append([word_dict[x] for x in row])\n",
    "\n",
    "X_train = padding(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_embed(numerical_list, embed_size):\n",
    "    result = nd.zeros((len(numerical_list), embed_size), ctx=ctx)\n",
    "    for i, idx in enumerate(numerical_list):\n",
    "        if(idx == -1):\n",
    "            continue\n",
    "        if(inv_word_dict[idx] not in model_words):\n",
    "            result[i] = nd.random_normal(shape=(1,embed_size), ctx=ctx) * .01\n",
    "            continue\n",
    "        if(inv_word_dict[idx] == '<SOS>'):\n",
    "            result[i] = nd.ones((1, embed_size), ctx=ctx)\n",
    "            continue\n",
    "        if(inv_word_dict[idx] == '<EOS>'):\n",
    "            result[i] = -nd.ones((1, embed_size), ctx=ctx)\n",
    "            continue\n",
    "        result[i] = model[inv_word_dict[idx]]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for row in X_train:\n",
    "    dataset.append(fast_embed(row[:-1], embed_size))\n",
    "\n",
    "temp = nd.zeros((len(dataset), dataset[0].shape[0], dataset[0].shape[1]), ctx=ctx)\n",
    "for i in range(len(temp)):\n",
    "    temp[i] = dataset[i]\n",
    "dataset = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sequences in dataset:  3610\n",
      "# of batches:  56\n",
      "Shape of data set:  (56L, 19L, 64L, 300L)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "seq_length = len(dataset[0])\n",
    "print('# of sequences in dataset: ', len(dataset))\n",
    "num_batches = len(dataset) // batch_size\n",
    "print('# of batches: ', num_batches)\n",
    "train_data = dataset[:num_batches*batch_size].reshape((batch_size, num_batches, seq_length, embed_size))\n",
    "train_data = nd.swapaxes(train_data, 0, 1)\n",
    "train_data = nd.swapaxes(train_data, 1, 2)\n",
    "print('Shape of data set: ', train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_label = []\n",
    "for i in X_train:\n",
    "    X_label.append(i[1:])\n",
    "    \n",
    "labels = []\n",
    "for row in X_label:\n",
    "    labels.append(one_hots(row, vocab_size))\n",
    "\n",
    "temp = nd.zeros((len(labels), labels[0].shape[0], labels[0].shape[1]), ctx=ctx)\n",
    "for i in range(len(temp)):\n",
    "    temp[i] = labels[i]\n",
    "labels = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data set:  (56L, 19L, 64L, 6694L)\n"
     ]
    }
   ],
   "source": [
    "train_labels = labels[:num_batches*batch_size].reshape((batch_size, num_batches, seq_length, vocab_size))\n",
    "train_labels = nd.swapaxes(train_labels, 0, 1)\n",
    "train_labels = nd.swapaxes(train_labels, 1, 2)\n",
    "print('Shape of data set: ', train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For training model, run this cell (For testing only, run below cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = embed_size\n",
    "num_hidden = 128\n",
    "num_outputs = vocab_size\n",
    "Wxh_model_2 = nd.random_normal(shape=(num_inputs,num_hidden), ctx=ctx) * .01\n",
    "Whh_model_2 = nd.random_normal(shape=(num_hidden,num_hidden), ctx=ctx) * .01\n",
    "bh_model_2 = nd.random_normal(shape=num_hidden, ctx=ctx) * .01\n",
    "Why_model_2 = nd.random_normal(shape=(num_hidden,num_outputs), ctx=ctx) * .01\n",
    "by_model_2 = nd.random_normal(shape=num_outputs, ctx=ctx) * .01\n",
    "\n",
    "params_model_2 = [Wxh_model_2, Whh_model_2, bh_model_2, Why_model_2, by_model_2]\n",
    "\n",
    "for param in params_model_2:\n",
    "    param.attach_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For simply testing without training run this cell instead of above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "num_inputs = embed_size\n",
    "num_hidden = 128\n",
    "num_outputs = vocab_size\n",
    "\n",
    "Wxh_model_2 = mxnet.ndarray.load(\"./weights/Wxh_model_2\")[0]\n",
    "Whh_model_2 = mxnet.ndarray.load(\"./weights/Whh_model_2\")[0]\n",
    "bh_model_2 = mxnet.ndarray.load(\"./weights/bh_model_2\")[0]\n",
    "Why_model_2 = mxnet.ndarray.load(\"./weights/Why_model_2\")[0]\n",
    "by_model_2 = mxnet.ndarray.load(\"./weights/by_model_2\")[0]\n",
    "\n",
    "params_model_2 = [Wxh_model_2, Whh_model_2, bh_model_2, Why_model_2, by_model_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rnn_model_2(inputs, state):\n",
    "    outputs = []\n",
    "    h = state\n",
    "    for X in inputs:\n",
    "        h_linear = nd.dot(X, Wxh_model_2) + nd.dot(h, Whh_model_2) + bh_model_2\n",
    "        h = nd.tanh(h_linear)\n",
    "        yhat_linear = nd.dot(h, Why_model_2) + by_model_2\n",
    "        yhat = softmax(yhat_linear)\n",
    "        outputs.append(yhat)\n",
    "    return (outputs, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_last_word_2(in_dataset, ground_data):\n",
    "    true = 0\n",
    "    count = 0\n",
    "    predicted = []\n",
    "    for i in range(len(in_dataset)):\n",
    "        if(len(in_dataset[i]) <= 3):\n",
    "            continue\n",
    "        state = nd.zeros(shape=(num_hidden), ctx=ctx)\n",
    "        data_one_hot = in_dataset[i]\n",
    "        with autograd.record():\n",
    "            outputs, state = simple_rnn_model_2(data_one_hot, state)\n",
    "        true += int(outputs[-3].asnumpy().argmax() == ground_data[i][-3])\n",
    "        count+=1\n",
    "\n",
    "    print(true, count, true/float(count)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "### (For simply testing the pretrained model, please run below cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "for row in X_test:\n",
    "    test_dataset.append(fast_embed(row[:-1], embed_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 236.27257013320923\n",
      "Epoch 1. Loss: 201.95642232894897\n",
      "Epoch 2. Loss: 197.75856757164001\n",
      "Epoch 3. Loss: 195.5913918018341\n",
      "Epoch 4. Loss: 194.40653133392334\n",
      "Epoch 5. Loss: 194.1512849330902\n",
      "Epoch 6. Loss: 193.5118727684021\n",
      "Epoch 7. Loss: 192.55285239219666\n",
      "Epoch 8. Loss: 192.4698624610901\n",
      "Epoch 9. Loss: 192.3498260974884\n",
      "Epoch 10. Loss: 191.37185883522034\n",
      "Epoch 11. Loss: 191.59667539596558\n",
      "Epoch 12. Loss: 189.77130699157715\n",
      "Epoch 13. Loss: 189.27198696136475\n",
      "Epoch 14. Loss: 187.38207125663757\n",
      "Epoch 15. Loss: 187.6641161441803\n",
      "Epoch 16. Loss: 184.74856209754944\n",
      "Epoch 17. Loss: 184.12769317626953\n",
      "Epoch 18. Loss: 182.73721504211426\n",
      "Epoch 19. Loss: 181.81715846061707\n",
      "Epoch 20. Loss: 180.76193833351135\n",
      "Epoch 21. Loss: 179.65906429290771\n",
      "Epoch 22. Loss: 179.03691339492798\n",
      "Epoch 23. Loss: 178.38765382766724\n",
      "Epoch 24. Loss: 177.73265075683594\n",
      "35 684 5.11695906433\n",
      "Epoch 25. Loss: 177.01154327392578\n",
      "Epoch 26. Loss: 176.49986004829407\n",
      "Epoch 27. Loss: 175.9004726409912\n",
      "Epoch 28. Loss: 175.28791189193726\n",
      "Epoch 29. Loss: 174.64952993392944\n",
      "Epoch 30. Loss: 174.05608415603638\n",
      "Epoch 31. Loss: 173.51066088676453\n",
      "Epoch 32. Loss: 172.94551467895508\n",
      "Epoch 33. Loss: 172.39762496948242\n",
      "Epoch 34. Loss: 171.84159898757935\n",
      "Epoch 35. Loss: 171.28264617919922\n",
      "Epoch 36. Loss: 170.72934412956238\n",
      "Epoch 37. Loss: 170.18810606002808\n",
      "Epoch 38. Loss: 169.68415021896362\n",
      "Epoch 39. Loss: 169.16228675842285\n",
      "Epoch 40. Loss: 168.59338212013245\n",
      "Epoch 41. Loss: 168.12763953208923\n",
      "Epoch 42. Loss: 167.5502724647522\n",
      "Epoch 43. Loss: 167.0861895084381\n",
      "Epoch 44. Loss: 166.5695357322693\n",
      "Epoch 45. Loss: 166.00597429275513\n",
      "Epoch 46. Loss: 165.6754674911499\n",
      "Epoch 47. Loss: 165.02735543251038\n",
      "Epoch 48. Loss: 164.64969682693481\n",
      "Epoch 49. Loss: 164.09094142913818\n",
      "41 684 5.99415204678\n",
      "Epoch 50. Loss: 163.6004855632782\n",
      "Epoch 51. Loss: 163.10998821258545\n",
      "Epoch 52. Loss: 162.66560339927673\n",
      "Epoch 53. Loss: 162.1724636554718\n",
      "Epoch 54. Loss: 161.69479060173035\n",
      "Epoch 55. Loss: 161.16031193733215\n",
      "Epoch 56. Loss: 160.80973291397095\n",
      "Epoch 57. Loss: 160.29324293136597\n",
      "Epoch 58. Loss: 159.81138968467712\n",
      "Epoch 59. Loss: 159.3150451183319\n",
      "Epoch 60. Loss: 158.82189631462097\n",
      "Epoch 61. Loss: 158.43608379364014\n",
      "Epoch 62. Loss: 158.0078625679016\n",
      "Epoch 63. Loss: 157.4507236480713\n",
      "Epoch 64. Loss: 156.94182205200195\n",
      "Epoch 65. Loss: 156.45910811424255\n",
      "Epoch 66. Loss: 156.17318558692932\n",
      "Epoch 67. Loss: 155.64953708648682\n",
      "Epoch 68. Loss: 155.1217110157013\n",
      "Epoch 69. Loss: 154.7734649181366\n",
      "Epoch 70. Loss: 154.32360434532166\n",
      "Epoch 71. Loss: 153.86991834640503\n",
      "Epoch 72. Loss: 153.5260272026062\n",
      "Epoch 73. Loss: 153.0653727054596\n",
      "Epoch 74. Loss: 152.6434841156006\n",
      "50 684 7.30994152047\n",
      "Epoch 75. Loss: 152.08432865142822\n",
      "Epoch 76. Loss: 151.7514088153839\n",
      "Epoch 77. Loss: 151.35507988929749\n",
      "Epoch 78. Loss: 150.90402364730835\n",
      "Epoch 79. Loss: 150.47326517105103\n",
      "Epoch 80. Loss: 150.08178567886353\n",
      "Epoch 81. Loss: 149.72690987586975\n",
      "Epoch 82. Loss: 149.15511655807495\n",
      "Epoch 83. Loss: 148.63209009170532\n",
      "Epoch 84. Loss: 148.26484727859497\n",
      "Epoch 85. Loss: 147.93636775016785\n",
      "Epoch 86. Loss: 147.5915069580078\n",
      "Epoch 87. Loss: 146.96010780334473\n",
      "Epoch 88. Loss: 146.46227955818176\n",
      "Epoch 89. Loss: 146.23313403129578\n",
      "Epoch 90. Loss: 145.86476254463196\n",
      "Epoch 91. Loss: 145.1546995639801\n",
      "Epoch 92. Loss: 144.89503717422485\n",
      "Epoch 93. Loss: 144.58520317077637\n",
      "Epoch 94. Loss: 144.24101853370667\n",
      "Epoch 95. Loss: 143.7258095741272\n",
      "Epoch 96. Loss: 143.32719469070435\n",
      "Epoch 97. Loss: 142.87060952186584\n",
      "Epoch 98. Loss: 142.54817152023315\n",
      "Epoch 99. Loss: 142.009352684021\n",
      "47 684 6.87134502924\n",
      "Epoch 100. Loss: 141.63374614715576\n",
      "Epoch 101. Loss: 141.33033919334412\n",
      "Epoch 102. Loss: 140.96948766708374\n",
      "Epoch 103. Loss: 140.61144733428955\n",
      "Epoch 104. Loss: 140.13450455665588\n",
      "Epoch 105. Loss: 139.77001810073853\n",
      "Epoch 106. Loss: 139.22849130630493\n",
      "Epoch 107. Loss: 139.08834290504456\n",
      "Epoch 108. Loss: 138.65496182441711\n",
      "Epoch 109. Loss: 138.05484771728516\n",
      "Epoch 110. Loss: 137.91489958763123\n",
      "Epoch 111. Loss: 137.40066647529602\n",
      "Epoch 112. Loss: 137.23745465278625\n",
      "Epoch 113. Loss: 136.74459719657898\n",
      "Epoch 114. Loss: 136.17883133888245\n",
      "Epoch 115. Loss: 135.824467420578\n",
      "Epoch 116. Loss: 135.50655484199524\n",
      "Epoch 117. Loss: 135.3074312210083\n",
      "Epoch 118. Loss: 134.55879306793213\n",
      "Epoch 119. Loss: 134.3142499923706\n",
      "Epoch 120. Loss: 133.99335598945618\n",
      "Epoch 121. Loss: 133.60580801963806\n",
      "Epoch 122. Loss: 133.20575785636902\n",
      "Epoch 123. Loss: 132.90479636192322\n",
      "Epoch 124. Loss: 132.58824276924133\n",
      "47 684 6.87134502924\n",
      "Epoch 125. Loss: 131.94051098823547\n",
      "Epoch 126. Loss: 131.90174221992493\n",
      "Epoch 127. Loss: 131.5690438747406\n",
      "Epoch 128. Loss: 131.09798645973206\n",
      "Epoch 129. Loss: 130.72155928611755\n",
      "Epoch 130. Loss: 130.38372898101807\n",
      "Epoch 131. Loss: 129.9275918006897\n",
      "Epoch 132. Loss: 129.39963364601135\n",
      "Epoch 133. Loss: 129.3659417629242\n",
      "Epoch 134. Loss: 128.50320506095886\n",
      "Epoch 135. Loss: 128.47138929367065\n",
      "Epoch 136. Loss: 128.39567399024963\n",
      "Epoch 137. Loss: 127.85768795013428\n",
      "Epoch 138. Loss: 127.465891122818\n",
      "Epoch 139. Loss: 127.2368381023407\n",
      "Epoch 140. Loss: 126.59422826766968\n",
      "Epoch 141. Loss: 126.70543432235718\n",
      "Epoch 142. Loss: 126.12120962142944\n",
      "Epoch 143. Loss: 125.56057024002075\n",
      "Epoch 144. Loss: 125.25779914855957\n",
      "Epoch 145. Loss: 124.98969435691833\n",
      "Epoch 146. Loss: 124.90552520751953\n",
      "Epoch 147. Loss: 124.08777117729187\n",
      "Epoch 148. Loss: 124.04912436008453\n",
      "Epoch 149. Loss: 123.9387059211731\n",
      "42 684 6.14035087719\n"
     ]
    }
   ],
   "source": [
    "epochs = 150\n",
    "\n",
    "learning_rate = 0.9\n",
    "\n",
    "# state = nd.zeros(shape=(batch_size, num_hidden), ctx=ctx)\n",
    "for e in range(epochs):\n",
    "    over_loss = 0\n",
    "    state = nd.zeros(shape=(batch_size, num_hidden), ctx=ctx)\n",
    "    for i in range(num_batches):\n",
    "        data_one_hot = train_data[i]\n",
    "        label_one_hot = train_labels[i]\n",
    "        with autograd.record():\n",
    "            outputs, state = simple_rnn_model_2(data_one_hot, state)\n",
    "            loss = average_ce_loss(outputs, label_one_hot)\n",
    "            loss.backward()\n",
    "        SGD(params_model_2, learning_rate)\n",
    "        over_loss += np.mean(loss.asnumpy()[0])\n",
    "    print(\"Epoch %s. Loss: %s\" % (e, over_loss))\n",
    "    if ((e+1) % 25 == 0):\n",
    "        test_last_word_2(test_dataset, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Results: (True, number_of_samples, percentage)\n",
      "612 3608 16.9623059867\n"
     ]
    }
   ],
   "source": [
    "train_dataset = []\n",
    "for row in X_train_list:\n",
    "    train_dataset.append(fast_embed(row[:-1], embed_size))\n",
    "    \n",
    "print(\"Train Results: (True, number_of_samples, percentage)\")\n",
    "test_last_word_2(train_dataset, X_train_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: (True, number_of_samples, percentage)\n",
      "40 684 5.84795321637\n"
     ]
    }
   ],
   "source": [
    "test_dataset = []\n",
    "for row in X_test:\n",
    "    test_dataset.append(fast_embed(row[:-1], embed_size))\n",
    "\n",
    "print(\"Test Results: (True, number_of_samples, percentage)\")\n",
    "test_last_word_2(test_dataset, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model parameters (please don't run while testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxnet.ndarray.save(\"Wxh_model_2\", Wxh_model_2)\n",
    "mxnet.ndarray.save(\"Whh_model_2\", Whh_model_2)\n",
    "mxnet.ndarray.save(\"bh_model_2\", bh_model_2)\n",
    "mxnet.ndarray.save(\"Why_model_2\", Why_model_2)\n",
    "mxnet.ndarray.save(\"by_model_2\", by_model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"n't\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-0e8421d9a424>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n't\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: \"n't\""
     ]
    }
   ],
   "source": [
    "word_dict[\"n't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
